<html>
<head>
  <meta charset="utf-8">
  <title>HIVE: Evaluating the Human Interpretability of Visual Explanations</title>

  <link href='https://fonts.googleapis.com/css?family=Overpass' rel='stylesheet'>
  <link rel="stylesheet" href="font-awesome-4.7.0/css/font-awesome.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
  <link href="mainpage.css" rel="stylesheet">
</head>

<body>


<!-- Lab logo -->
<div class="row" style="text-align:center;padding:0;padding-top:20;padding-bottom:10;margin:0">
  <div class="container">
    <!--<img src="imgs/eccv2022.png" height="100px" style="vertical-align:middle">-->
    <!--<span style="font-size:32px;vertical-align:middle"><a href="https://visualai.princeton.edu" style="color:#ff8f00" target="_blank">Princeton Visual AI Lab</a></span> -->
  </div>
</div>


<!-- Authors -->
<div class="container-fluid">
  <div class="row">
    <h1><span style="font-size:40px;color:#333;font-weight:800">Understanding Explainability and Trust in AI</span></h1>
    <div class="authors">
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~suhk/" style="color:#1075bc" target="new">Sunnie S. Y. Kim</a><sup>1</sup></span>
      &nbsp;
      <span style="font-size:18px"><a href="http://www.elizabethannewatkins.com/" style="color:#1075bc" target="new">Elizabeth Anne Watkins</a><sup>2</sup></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~olgarus/" style="color:#1075bc" target="new">Olga Russakovsky</a><sup>1</sup></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://ruthcfong.github.io/" style="color:#1075bc" target="new">Ruth Fong</a><sup>1</sup></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.andresmh.com/" style="color:#1075bc" target="new">Andrés Monroy-Hernández</a><sup>1</sup></span>
      <br>
      <span style="font-size:18px"><sup>1</sup>Princeton University &nbsp;&nbsp;&nbsp; <sup>2</sup>Intel Labs<br><br></span>
    </div>
  </div>
</div>

<!-- Figure 1 -->
<div class="row" style="text-align:center;padding:0;margin:0">
  <img src="imgs/explanations.png" style="width:55%">
</div>
<div class="container">
  (Top left) Heatmap explanations by GradCAM and BagNet highlight decision-relevant image regions.
  (Bottom left) Prototype-based explanations by ProtoPNet and ProtoTree match image regions to prototypical parts learned during training.
  This schematic is much simpler than actual explanations.
  (Right) Actual ProtoPNet explanation example from the original paper.
  <i>
    While existing evaluation methods typically apply to only one explanation form,
    HIVE evaluates and compares diverse interpretability methods.
  </i>
</div>


&nbsp;

<div class="container-fluid"></div>


<!-- Icons -->
<div class="container">
  <div class="row">
    <div class="col-lg-0 col-md-0 col-sm-0"></div>

    <div class="col-xs-1 col-xs-offset-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://arxiv.org/abs/2112.03184" target="_blank">
          <i class="fa fa-4x fa-file-text-o text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">Paper</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://github.com/princetonvisualai/HIVE/blob/main/materials/HIVE_suppmat.pdf" target="_blank">
          <i class="fa fa-4x fa-file-text-o text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">Supplement</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://drive.google.com/file/d/1nOYfy_0e61cGGDzwreCI4IUly1VbTgur/view?usp=sharing" target="_blank">
          <i class="fa fa-4x fa-file-text-o text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">Extended Abstract</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://github.com/princetonvisualai/HIVE/blob/main/materials/1511.pdf" target="_blank">
          <i class="fa fa-4x fa-list-alt text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">Poster</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://youtu.be/BDlFb1CFQRQ" target="_blank">
          <i class="fa fa-4x fa-youtube-play text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">2min Talk</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://youtu.be/Wm-r-jtSrF8" target="_blank">
          <i class="fa fa-4x fa-youtube-play text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">4min Talk</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://youtu.be/7uysq-qAtr4" target="_blank">
          <i class="fa fa-4x fa-youtube-play text-primary mb-3"></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">8min Talk</h4>
      </div>
    </div>

    <div class="col-xs-1 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://github.com/princetonvisualai/HIVE" target="_blank">
          <i class="fa fa-4x fa-github text-primary mb-3 "></i>
        </a>
        <h4 class="mb-3" style="font-size:14px">Code</h4>
        </div>
    </div>

  </div>
</div>


<!-- News -->
<div class="container">
  <h2>News</h2>
  01/2023: <a href="https://arxiv.org/abs/2210.03735" target="_blank">"Help Me Help the AI"</a> has been conditionally accepted to <a href="https://chi2023.acm.org/" target="_blank">CHI 2023</a>!
  
  
  <br />
  <br />
  
  10/2022: Check out our follow-up paper <a href="https://arxiv.org/abs/2210.03735" target="_blank">"Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction</a>.
  We interviewed 20 end-users of the Merlin bird identification app and studied (1) what explainability needs they have, (2) how they intend to use explanations of the AI system's predictions, and (3) how they perceive existing XAI approaches (heatmap, example, concept, prototype).

  <br />
  <br />
  
  07/2022: HIVE has been accepted to <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>. The camera-ready version is available <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720277.pdf" target="_blank">here</a>!

  <br />
  <br />

  06/2022: We presented HIVE at the <a href="https://xai4cv.github.io/workshop" target="_blank">CVPR 2022 Explainable AI for Computer Vision Workshop</a> (spotlight talk &#38; poster) and
  the <a href="https://sites.google.com/view/wicvcvpr2022/home" target="_blank">CVPR 2022 Women in Computer Vision Workshop</a> (poster).

  <br />
  <br />

  05/2022: We presented HIVE at the <a href="https://hcxai.jimdosite.com/" target="_blank">CHI 2022 Human-Centered Explainable AI Workshop</a> (poster).
</div>


<!-- Abstract -->
<div class="container">
  <h2>Abstract</h2>
  As AI technology is increasingly applied to high-impact, high-risk domains,
  there have been a number of new methods aimed at making AI models more human interpretable.
  Despite the recent growth of interpretability work, there is a lack of systematic
  evaluation of proposed techniques. In this work, we introduce HIVE
  (Human Interpretability of Visual Explanations), a novel human evaluation
  framework that assesses the utility of explanations to human users in
  AI-assisted decision making scenarios, and enables falsifiable hypothesis testing,
  cross-method comparison, and human-centered evaluation of visual interpretability methods.
  To the best of our knowledge, this is the first work of its kind. Using HIVE, we conduct
  IRB-approved human studies with nearly 1000 participants and evaluate four methods
  that represent the diversity of computer vision interpretability works: GradCAM, BagNet,
  ProtoPNet, and ProtoTree. Our results suggest that explanations engender human trust,
  even for incorrect predictions, yet are not distinct enough for users to distinguish
  between correct and incorrect predictions. We open-source HIVE to enable future studies
  and encourage more human-centered approaches to interpretability research.
</div>

<!-- Citation -->
<div class="container">
  <h2>Citation</h2>
  <pre><code>
    @inproceedings{Kim2022HIVE,
      author = {Sunnie S. Y. Kim and Nicole Meister and Vikram V. Ramaswamy and Ruth Fong and Olga Russakovsky},
      title = {{HIVE}: Evaluating the Human Interpretability of Visual Explanations},
      booktitle = {European Conference on Computer Vision (ECCV)},
      year = {2022}
    }
  </code></pre>
</div>


<!-- Method -->
<div class="container">
  <h2>HIVE (Human Interpretability of Visual Explanations)</h2>

  We propose HIVE, a novel human evaluation framework for visual interpretability methods.
  Through careful design, HIVE allows for
  <i>falsifiable hypothesis testing</i> regarding the utility of explanations for identifying model errors,
  <i>cross-method comparison</i> between different interpretability techniques,
  and <i>human-centered evaluation</i> for understanding the practical effectiveness of interpretability.

  <br />
  <br />

  In particular, we focus on AI-assisted decision making scenarios where humans use an AI (image classification) model
  and an interpretability method to make decisions about whether the model prediction is correct or more generally
  about whether to use the model. We evaluate how useful a given interpretability
  method is in these scenarios through the following tasks.

  <br />
  <br />

  <img src="imgs/protopnet_agreement.png" style="width:50%">

  <br />
  <br />

  First, we evaluate interpretability methods on a simple <i>agreement</i> task, where we present users with a single
  model prediction-explanation pair for a given image and ask how confident they are in the prediction.
  This task simulates a common decision making setting and is close to existing evaluation schemes that consider
  a model’s top-1 prediction and an explanation for it. See above for the ProtoPNet evaluation UI.

  <br />
  <br />

  <img src="imgs/gradcam_distinction.png" style="width:50%">

  <br />
  <br />

  However, it has been previously observed that users tend to believe in model predictions when given explanations for them.
  Hence, we evaluate methods on a <i>distinction</i> task to mitigate the effect of such confirmation bias in interpretability evaluation.
  Here we simultaneously show four prediction-explanation pairs and ask users to identify the correct prediction based on the
  provided explanations. This task measures how well explanations can help users distinguish between correct and incorrect predictions.
  See above for the GradCAM evaluation UI.

  <br />
  <br />

  <img src="imgs/studydesign.png" style="width:50%">

  <br />
  <br />

  In summary, HIVE consists of the following steps:
  We first introduce the study and the interpretability method to be evaluated.
  Next, we show a preview of the evaluation task and provide example explanations
  for one correct and one incorrect model prediction to give participants appropriate references.
  Afterwards, participants complete the evaluation task.
  Throughout the study, we also ask subjective evaluation and user preference questions
  to make the most out of the human studies.
  Our study design was approved by our university’s Institutional Review Board (IRB).

</div>



<!-- Key Findings -->
<div class="container">
  <h2>Key Findings</h2>

  1. When provided explanations, participants tend to believe that the model predictions are correct, revealing an issue of <i>confirmation bias</i>.

  <br />
  <br />

  2. When given multiple model predictions and explanations, participants struggle to distinguish
  between correct and incorrect predictions based on the explanations.
  This result suggests that interpretability methods need to be improved to be reliably useful for AI-assisted decision making.

  <br />
  <br />

  3. There exists a gap between the similarity judgments of humans and prototype-based models
  which can hurt the quality of their interpretability.

  <br />
  <br />

  4. Participants prefer to use a model with explanations over a baseline model without explanations.
  To switch their preference, they require the baseline model to have +6.2% to +10.9% higher accuracy.

  <br />
  <br />

  Please see the <a href="https://arxiv.org/abs/2112.03184" target="_blank">full paper</a> for details.

</div>


<!-- Related Work -->
<div class="container" >
  <h2>Related Work</h2>
  <div>
  Below are some papers related to our work. We discuss them in more detail in the related work section of our paper.
  </div>

  &nbsp;

  <div>
    [1] <a href="https://arxiv.org/abs/1610.02391" target="_blank">Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization.</a>
    Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra.
    IJCV 2019.
  </div>

  &nbsp;

  <div>
    [2] <a href="https://arxiv.org/abs/1904.00760" target="_blank">Approximating CNNs with Bag-of-local-Features Models Works Surprisingly Well on ImageNet.</a>
    Wieland Brendel, Matthias Bethge.
    ICLR 2019.
  </div>

  &nbsp;

  <div>
    [3] <a href="https://arxiv.org/abs/1806.10574" target="_blank">This Looks Like That: Deep Learning for Interpretable Image Recognition.</a>
    Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin.
    NeurIPS 2019.
  </div>

  &nbsp;

  <div>
    [4] <a href="https://arxiv.org/abs/2105.02968" target="_blank">This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks.</a>
    Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler.
    ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI.
  </div>

  &nbsp;

  <div>
    [5] <a href="https://arxiv.org/abs/2012.02046" target="_blank">Neural Prototype Trees for Interpretable Fine-grained Image Recognition.</a>
    Meike Nauta, Ron van Bree, Christin Seifert.
    CVPR 2021.
  </div>

  &nbsp;

  <div>
    [6] <a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep Residual Learning for Image Recognition.</a>
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.
    CVPR 2016.
  </div>

</div>

<!-- Acknowledgements -->
<div class="container">
  <h2>Acknowledgements</h2>

  This material is based upon work partially supported by the National Science Foundation (NSF)
  under Grant No. 1763642. Any opinions, findings, and conclusions or recommendations expressed
  in this material are those of the author(s) and do not necessarily reflect the views of the NSF.
  We also acknowledge support from the Princeton SEAS Howard B. Wentz, Jr. Junior Faculty Award (OR),
  Princeton SEAS Project X Fund (RF, OR), Open Philanthropy (RF, OR), and Princeton SEAS and ECE Senior Thesis Funding (NM).
  We thank the authors of [1, 2, 3, 4, 5] for open-sourcing their code and the authors of [2, 4, 5, 6] for sharing their trained models.
  We also thank the AMT workers who participated in our studies, anonymous reviewers who provided thoughtful feedback,
  and Princeton Visual AI Lab members (especially Dora Zhao, Kaiyu Yang, and Angelina Wang) who tested our user
  interface and provided helpful suggestions.

</div>

<div class="container" >
  <h2>Contact</h2>
  <div><a href="https://www.cs.princeton.edu/~suhk/" target="_blank">Sunnie S. Y. Kim</a> (sunniesuhyoung@princeton.edu)</div>
</div>

<div id="footer">
</div>


</body>
</html>
